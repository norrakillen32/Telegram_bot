import json
import re
from typing import Tuple, Optional, Dict, List, Any
import difflib
from enum import Enum

class TextPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', ' ', text)  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'\s+', ' ', text)      # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text
    
    @staticmethod
    def extract_keywords(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        stop_words = {
            '–∫–∞–∫', '–≥–¥–µ', '—á—Ç–æ', '–∫—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            '–º–Ω–µ', '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è',
            '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ—ë', '—Å–≤–æ–∏',
            '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—ç—Ç–æ—Ç',
            '–≤–æ—Ç', '—Ç—É—Ç', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É–¥–∞',
            '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '–≤–æ–æ–±—â–µ', '—Å–æ–≤—Å–µ–º',
            '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '—Ö–æ—á—É', '—Ö–æ—Ç–µ–ª'
        }
        
        words = text.split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        return keywords
    
    @staticmethod
    def get_word_variations(word: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Å–ª–æ–≤–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫"""
        variations = [word]
        
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        common_typos = {
            '–∞': ['–æ'], '–æ': ['–∞'], '–µ': ['—ç'], '–∏': ['–π', '—ã'],
            '—Ç': ['—Ç—Ç', '–¥'], '–ø': ['–ø–ø', '–±'], '–∫': ['–∫–∫', '–≥'],
            '—Å': ['—Å—Å', '–∑'], '–≤': ['–≤–≤', '—Ñ']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –∑–∞–º–µ–Ω–æ–π –ø–æ—Ö–æ–∂–∏—Ö –±—É–∫–≤
        for i, char in enumerate(word):
            if char in common_typos:
                for replacement in common_typos[char]:
                    variation = word[:i] + replacement + word[i+1:]
                    variations.append(variation)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏/–ª–∏—à–Ω–∏–º–∏ –±—É–∫–≤–∞–º–∏ (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤)
        if len(word) > 3:
            # –ü—Ä–æ–ø—É—Å–∫ –æ–¥–Ω–æ–π –±—É–∫–≤—ã
            for i in range(len(word)):
                variations.append(word[:i] + word[i+1:])
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –±—É–∫–≤—ã (–ø–æ–≤—Ç–æ—Ä)
            for i in range(len(word)-1):
                if word[i] == word[i+1]:
                    variations.append(word[:i] + word[i+1:])
        
        return list(set(variations))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏

class FuzzySearcher:
    """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    @staticmethod
    def fuzzy_ratio(text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        # –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        base_ratio = difflib.SequenceMatcher(None, text1, text2).ratio()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        words1 = text1.split()
        words2 = text2.split()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
        word_overlap = len(set(words1) & set(words2)) / max(len(set(words1)), 1)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        first_letter_score = 0
        if words1 and words2:
            if words1[0][0] == words2[0][0]:
                first_letter_score = 0.2
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        fuzzy_score = (base_ratio * 0.6) + (word_overlap * 0.3) + (first_letter_score * 0.1)
        
        return fuzzy_score
    
    @staticmethod
    def find_best_fuzzy_match(query: str, candidates: List[str], threshold: float = 0.5) -> Tuple[Optional[str], float]:
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –Ω–µ—á–µ—Ç–∫–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        if not candidates:
            return None, 0.0
        
        best_match = None
        best_score = 0.0
        
        for candidate in candidates:
            score = FuzzySearcher.fuzzy_ratio(query, candidate)
            if score > best_score:
                best_score = score
                best_match = candidate
        
        if best_score >= threshold:
            return best_match, best_score
        
        return None, 0.0
    
    @staticmethod
    def soundex_rus(word: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Soundex –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        if not word:
            return ""
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã
        first_char = word[0].upper()
        
        # –ö–æ–¥—ã –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –±—É–∫–≤
        codes = {
            '–∞': '0', '–±': '1', '–≤': '2', '–≥': '3', '–¥': '4', '–µ': '0', '—ë': '0',
            '–∂': '1', '–∑': '2', '–∏': '0', '–π': '0', '–∫': '3', '–ª': '4', '–º': '5',
            '–Ω': '6', '–æ': '0', '–ø': '1', '—Ä': '2', '—Å': '3', '—Ç': '4', '—É': '0',
            '—Ñ': '1', '—Ö': '2', '—Ü': '3', '—á': '4', '—à': '5', '—â': '6', '—ä': '0',
            '—ã': '0', '—å': '0', '—ç': '0', '—é': '0', '—è': '0'
        }
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Å–ª–æ–≤–æ
        encoded = first_char
        
        for char in word[1:].lower():
            code = codes.get(char, '0')
            if code != '0' and (not encoded or encoded[-1] != code):
                encoded += code
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 4 —Å–∏–º–≤–æ–ª–æ–≤
        encoded = (encoded + '000')[:4]
        
        return encoded
    
    @staticmethod
    def soundex_match(query: str, target: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ Soundex"""
        query_soundex = FuzzySearcher.soundex_rus(query)
        target_soundex = FuzzySearcher.soundex_rus(target)
        
        return query_soundex == target_soundex

class KnowledgeBaseSearcher:
    """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, file_path: str = "knowledge_base.json"):
        self.file_path = file_path
        self.kb_data = self._load_knowledge_base()
        self.preprocessor = TextPreprocessor()
        self.fuzzy_searcher = FuzzySearcher()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.question_index = self._build_index()
    
    def _load_knowledge_base(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
                return data
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _build_index(self) -> Dict[str, List[Dict]]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        index = {}
        
        for item in self.kb_data:
            question = item.get('question', '')
            normalized = self.preprocessor.normalize_text(question)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = self.preprocessor.extract_keywords(normalized)
            for keyword in keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(item)
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ Soundex
            soundex = self.fuzzy_searcher.soundex_rus(question)
            if soundex not in index:
                index[soundex] = []
            index[soundex].append(item)
        
        return index
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        return self.fuzzy_searcher.fuzzy_ratio(text1, text2)
    
    def find_best_match(
        self, 
        user_question: str, 
        source_type: Optional[str] = None,
        threshold: float = 0.4  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
    ) -> Tuple[Optional[Dict], float]:
        """
        –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        """
        if not self.kb_data:
            return None, 0.0
        
        normalized_question = self.preprocessor.normalize_text(user_question)
        keywords = self.preprocessor.extract_keywords(normalized_question)
        
        best_item = None
        best_confidence = 0.0
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å (–±—ã—Å—Ç—Ä—ã–π)
        candidate_items = set()
        
        for keyword in keywords:
            if keyword in self.question_index:
                candidate_items.update(self.question_index[keyword])
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å, –∏—â–µ–º –≤–æ –≤—Å–µ–π –±–∞–∑–µ
        if not candidate_items:
            candidate_items = self.kb_data
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫
        query_variations = []
        for keyword in keywords[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            variations = self.preprocessor.get_word_variations(keyword)
            query_variations.extend(variations)
        
        for item in candidate_items:
            item_question = item.get('question', '')
            item_source = item.get('source', 'manual')
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if source_type and item_source != source_type:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –±–∞–∑—ã
            normalized_item = self.preprocessor.normalize_text(item_question)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —á–µ—Ä–µ–∑ –Ω–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
            similarity = self._calculate_similarity(normalized_question, normalized_item)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ Soundex
            soundex_match = self.fuzzy_searcher.soundex_match(
                normalized_question[:10],  # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                normalized_item[:10]
            )
            
            if soundex_match:
                similarity = max(similarity, 0.6)  # –ü–æ–≤—ã—à–∞–µ–º score –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –ø–æ Soundex
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            for variation in query_variations[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫
                if variation in normalized_item:
                    similarity = max(similarity, 0.55)  # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å
                    break
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            item_keywords = self.preprocessor.extract_keywords(normalized_item)
            common_keywords = set(keywords) & set(item_keywords)
            keyword_overlap = len(common_keywords) / max(len(keywords), 1)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            confidence = (similarity * 0.6) + (keyword_overlap * 0.4)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_item = item
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–∏–∂–µ –¥–ª—è —É—á–µ—Ç–∞ –æ–ø–µ—á–∞—Ç–æ–∫)
        if best_confidence >= threshold:
            return best_item, best_confidence
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º –≤–æ–ø—Ä–æ—Å–∞
        if len(keywords) > 1:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            for item in self.kb_data:
                if source_type and item.get('source') != source_type:
                    continue
                    
                item_text = self.preprocessor.normalize_text(item.get('question', ''))
                matches = sum(1 for kw in keywords if kw in item_text)
                
                if matches >= 2:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    confidence = matches / len(keywords)
                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_item = item
        
        if best_confidence >= threshold * 0.8:  # –ï—â–µ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
            return best_item, best_confidence
        
        return None, 0.0
    
    def find_by_exact_question(
        self, 
        question: str, 
        source_type: Optional[str] = None
    ) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É"""
        normalized_question = self.preprocessor.normalize_text(question)
        
        for item in self.kb_data:
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            item_source = item.get('source', 'manual')
            
            if source_type and item_source != source_type:
                continue
            
            if item_question == normalized_question:
                return item
        
        return None

class IntentClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    def __init__(self):
        self.intents = {
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', 'hello', 'hi', '–Ω–∞—á–∞—Ç—å', '–ø—Ä–∏–≤'],
            'farewell': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', '–≤—ã—Ö–æ–¥', '–∑–∞–∫–æ–Ω—á–∏—Ç—å', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∫', '–≤—Å–µ–≥–æ'],
            'help': ['–ø–æ–º–æ—â—å', '–ø–æ–º–æ–≥–∏', '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å', '–∫–æ–º–∞–Ω–¥—ã', '–ø–æ–¥—Å–∫–∞–∂–∏', '–ø–æ—Å–æ–≤–µ—Ç—É–π'],
            'question_1c': ['–∫–∞–∫', '–≥–¥–µ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–º–æ–∂–Ω–æ –ª–∏', '–∫–∞–∫–æ–π', '—á–µ–º'],
            'document': ['–Ω–∞–∫–ª–∞–¥–Ω–∞—è', '—Å—á–µ—Ç', '–∞–∫—Ç', '–¥–æ–≥–æ–≤–æ—Ä', '–æ—Ä–¥–µ—Ä', '–æ—Ç—á–µ—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç'],
            'operation': ['—Å–æ–∑–¥–∞—Ç—å', '—É–¥–∞–ª–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '–æ—Ç–º–µ–Ω–∏—Ç—å', '—Å–¥–µ–ª–∞—Ç—å', '–Ω–∞–ø–∏—Å–∞—Ç—å'],
            'search': ['–Ω–∞–π—Ç–∏', '–ø–æ–∏—Å–∫', '–∏—Å–∫–∞—Ç—å', '–≥–¥–µ –Ω–∞–π—Ç–∏', '–∫–∞–∫ –Ω–∞–π—Ç–∏', '–Ω–∞–π–¥–∏'],
            'button_click': ['button:', 'menu:', '–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–Ω–æ–ø–∫–∞']
        }
    
    def classify(self, text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        text_lower = text.lower()
        detected_intents = []
        
        for intent, keywords in self.intents.items():
            for keyword in keywords:
                if keyword in text_lower:
                    detected_intents.append(intent)
                    break
        
        return detected_intents if detected_intents else ['unknown']
    
    def is_button_click(self, text: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç—ã: "button:–Ω–∞–∫–ª–∞–¥–Ω—ã–µ" –∏–ª–∏ "menu:–æ—Ç—á–µ—Ç—ã"
        for prefix in ['button:', 'menu:']:
            if text_lower.startswith(prefix):
                parts = text_lower.split(':', 1)
                if len(parts) == 2:
                    return True, prefix.rstrip(':'), parts[1].strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (—Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫)
        button_patterns = [
            (['–Ω–∞–∂–∞—Ç—å –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–º–∏ –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂—ã –∫–Ω–æ–ø–∫—É', '–Ω–∞–∂–∞—Ç—å–∫–Ω–æ–ø–∫—É'], 'button'),
            (['–∫–ª–∏–∫ –ø–æ –∫–Ω–æ–ø–∫–µ', '–∫–ª–∏–∫–Ω—É—Ç—å –∫–Ω–æ–ø–∫—É', '–∫–ª–∏–∫ –ø–æ', '–∫–ª–∏–∫–Ω—É—Ç—å'], 'button'),
            (['–≤ –º–µ–Ω—é', '–º–µ–Ω—é', '–≤ —Ä–∞–∑–µ–¥–µ–ª', '—Ä–∞–∑–µ–¥–µ–ª'], 'menu'),
            (['—Ä–∞–∑–¥–µ–ª', '—Ä–∞–∑–¥–∏–ª', '—Ä–∞–¥–µ–ª'], 'menu')
        ]
        
        for patterns, source_type in button_patterns:
            for pattern in patterns:
                if pattern in text_lower:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    start_idx = text_lower.find(pattern) + len(pattern)
                    button_text = text_lower[start_idx:].strip()
                    if button_text:
                        return True, source_type, button_text
        
        return False, None, None

class ButtonHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –∫–Ω–æ–ø–æ–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self, kb_searcher: KnowledgeBaseSearcher):
        self.kb_searcher = kb_searcher
        self.preprocessor = TextPreprocessor()
    
    def handle_button_click(
        self, 
        source_type: str, 
        button_text: str
    ) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        print(f"üîò –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–æ–ø–∫–∏: source={source_type}, text='{button_text}'")
        
        normalized_button = self.preprocessor.normalize_text(button_text)
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        exact_match = self.kb_searcher.find_by_exact_question(
            normalized_button, 
            source_type=source_type
        )
        
        if exact_match:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
            return exact_match
        
        # 2. –ò—â–µ–º —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫ (–Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥)
        fuzzy_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            source_type=source_type,
            threshold=0.3  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–Ω–æ–ø–æ–∫
        )
        
        if fuzzy_match and confidence >= 0.3:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–µ—á–µ—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return fuzzy_match
        
        # 3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º source, –∏—â–µ–º –≤ –ª—é–±–æ–º source
        any_match, confidence = self.kb_searcher.find_best_match(
            normalized_button,
            threshold=0.35
        )
        
        if any_match:
            print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –¥—Ä—É–≥–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return any_match
        
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –∫–Ω–æ–ø–∫–∏ '{button_text}'")
        return None

class NLPEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π NLP-–¥–≤–∏–∂–æ–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()
        self.kb_searcher = KnowledgeBaseSearcher()
        self.button_handler = ButtonHandler(self.kb_searcher)
    
    def process_message(self, user_message: str) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        """
        print(f"\nüì® –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{user_message}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –Ω–∞–∂–∞—Ç–∏–µ–º –∫–Ω–æ–ø–∫–∏
        is_button_click, source_type, button_text = self.intent_classifier.is_button_click(
            user_message
        )
        
        if is_button_click and source_type and button_text:
            print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ –Ω–∞–∂–∞—Ç–∏–µ –∫–Ω–æ–ø–∫–∏: {source_type} -> '{button_text}'")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∫–Ω–æ–ø–∫—É
            kb_item = self.button_handler.handle_button_click(source_type, button_text)
            
            if kb_item:
                return {
                    'original_message': user_message,
                    'normalized_message': button_text,
                    'intents': ['button_click'],
                    'source_type': source_type,
                    'kb_answer': kb_item.get('answer'),
                    'kb_item': kb_item,
                    'kb_confidence': 1.0,
                    'has_kb_answer': True,
                    'is_button_click': True,
                    'is_fuzzy_match': False
                }
        
        # –û–±—ã—á–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        normalized = self.preprocessor.normalize_text(user_message)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        intents = self.intent_classifier.classify(normalized)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫
        kb_item, kb_confidence = self.kb_searcher.find_best_match(
            user_message, 
            threshold=0.35  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —ç—Ç–æ fuzzy match
        is_fuzzy_match = False
        if kb_item and kb_confidence < 0.7:
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ–≤—ã—Å–æ–∫–∞—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            original_question = kb_item.get('question', '')
            if original_question.lower() != normalized:
                is_fuzzy_match = True
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'original_message': user_message,
            'normalized_message': normalized,
            'intents': intents,
            'keywords': keywords,
            'kb_answer': kb_item.get('answer') if kb_item else None,
            'kb_item': kb_item,
            'kb_confidence': kb_confidence,
            'has_kb_answer': kb_item is not None,
            'is_button_click': False,
            'is_fuzzy_match': is_fuzzy_match
        }
        
        return result
    
    def get_final_answer(self, user_message: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        analysis = self.process_message(user_message)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        if analysis['has_kb_answer']:
            kb_item = analysis['kb_item']
            answer = kb_item.get('answer', '')
            
            # –î–ª—è –∫–Ω–æ–ø–æ–∫ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
            if analysis.get('is_button_click'):
                source = kb_item.get('source', '')
                button_text = kb_item.get('metadata', {}).get('button_text', '')
                
                if button_text and source in ['menu', 'button']:
                    header = f"üîò **{button_text}**\n\n"
                    return header + answer
            
            # –î–ª—è fuzzy match –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ
            confidence_percent = int(analysis['kb_confidence'] * 100)
            
            if analysis.get('is_fuzzy_match'):
                original_question = kb_item.get('question', '')
                return f"‚úÖ {answer}\n\n<i>(–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: '{original_question}'. –ù–∞–π–¥–µ–Ω–æ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
            else:
                return f"‚úÖ {answer}\n\n<i>(–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence_percent}%)</i>"
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        suggestions = self._get_search_suggestions(user_message)
        return f"ü§î <b>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.</b>\n\n{suggestions}"
    
    def _get_search_suggestions(self, query: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –ø–æ–∏—Å–∫—É"""
        normalized = self.preprocessor.normalize_text(query)
        keywords = self.preprocessor.extract_keywords(normalized)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–∞–∑–µ
        similar_questions = []
        
        for item in self.kb_searcher.kb_data[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10
            item_question = self.preprocessor.normalize_text(item.get('question', ''))
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            item_keywords = self.preprocessor.extract_keywords(item_question)
            common = set(keywords) & set(item_keywords)
            
            if len(common) >= 1 and item_question not in similar_questions:
                similar_questions.append(item_question)
            
            if len(similar_questions) >= 3:
                break
        
        suggestions = "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
        suggestions += "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é\n"
        suggestions += "2. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å\n"
        
        if similar_questions:
            suggestions += "3. –í–æ–∑–º–æ–∂–Ω–æ, –≤–∞–º –Ω—É–∂–µ–Ω –æ–¥–∏–Ω –∏–∑ —ç—Ç–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤:\n"
            for i, q in enumerate(similar_questions, 1):
                suggestions += f"   ‚Ä¢ {q}\n"
        
        suggestions += "4. –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É"
        
        return suggestions

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NLP-–¥–≤–∏–∂–∫–∞
nlp_engine = NLPEngine()
